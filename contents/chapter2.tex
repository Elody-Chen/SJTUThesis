% !TEX root = ../main.tex


\chapter{基于词嵌入的命名实体识别方案}

\section{嵌入方案介绍}

自然语言处理任务中的一个关键问题就是如何用数学语言去表示自然语言，并且其表示的优劣则会直接影响其下游任务结果的好坏。

\subsection{词嵌入(Word Embedding)}

\paragraph{One-Hot编码}
最简单且最容易实现的的表示方法之一就是给词编码。假如能够列出所有可能出现的词，我们只需要给每个词一个唯一的编码，
即可完成简单的词表示。例如一个词表中有12个词，按照拼音首字母排列如下：

\begin{table}[!hpt]
    \caption[示例词典编码对照表]{示例词典编码对照表}
    \label{tab:case_lookup_table}
    \centering
    \begin{tabular}{lr} \toprule
      词语  &   编码 \\ \midrule
      处理  &   1 \\
      的    &   2 \\
      方案  &   3 \\
      非常  &   4 \\
      命名  &   5 \\
      难以  &   6 \\
      识别  &   7 \\
      实体  &   8 \\
      是    &   9 \\
      学习  &   10 \\
      语言  &   11 \\
      自然  &   12 \\ \bottomrule
    \end{tabular}
\end{table}

“自然语言处理非常难以学习”这句话分词后的结果为(自然, 语言, 处理, 非常, 难以, 学习)用
表~\ref{tab:case_lookup_table}的编码表示就是：(12, 11, 1, 4, 6, 10)。

在使用机器学习算法做自然语言处理任务的时候，模型所接受的输入一般是定长的向量，
所以需要将上表中的所有编码转换为向量。One-Hot编码是使用与词表等长的向量来表示特定词语，向量中的元素只包含0或1。
每个词只有在它对应的位置上元素为1，其余都为0。对于表~\ref{tab:case_lookup_table}，
其对应的One-Hot向量如表~\ref{tab:case_one_hot_lookup_table}

\begin{table}[!hpt]
    \caption[示例词典One-Hot向量对照表]{示例词典One-Hot向量对照表}
    \label{tab:case_one_hot_lookup_table}
    \centering
    \begin{tabular}{lrl} \toprule
      词语  &   编码 &  One-Hot向量 \\ \midrule
      处理	&	1	&   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	\\
      的	&	2	&	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	\\
      方案	&	3	&	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	\\
      非常	&	4	&	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	\\
      命名	&	5	&	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	\\
      难以	&	6	&	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	\\
      识别	&	7	&	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	\\
      实体	&	8	&	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	\\
      是	&	9	&	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	\\
      学习	&	10	&	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	\\
      语言	&	11	&	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	\\
      自然	&	12	&	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	\\ \bottomrule
    \end{tabular}
\end{table}

这样表示虽然清晰，但却存在两个严重的问题：
\begin{enumerate}
    \item \textbf{向量维度过高并且高度稀疏}：自然语言处理任务中的词汇量一般非常大。
    例如中文或者英文的常用词有上万个，其他语言也类似。如此多的词语会导致One-Hot向量的维度非常大，
    计算时不仅浪费内存，计算量也极大。
    \item \textbf{信息量少，无法表征语义相似度}：自然语言的词与词之间可能存在一定的关系。
    如果我们用词向量的余弦相似度来衡量两个词的相关性，越接近1或者-1表示两个词语义越相近或者相反，
    越接近0表示语义约不相关。例如“漂亮”和“美丽”是一对近义词，它们的余弦相似度应该接近于1；
    “美丽”和“丑陋”是一对反义词，它们的余弦相似度应该接近于-1；而“美丽”和“手机”在语义上的关系较弱，
    其余弦相似度则应该接近于0。
\end{enumerate}


\paragraph{Word2Vec}

Word2Vec即word to vector，顾名思义就是将词转换为向量。该方法由\parencite{mikolov2013efficient}首创，
主要有以下特点：
\begin{enumerate}
    \item \textbf{算法效率高}：可以在百万数量级的词典和上亿规模的数据上训练；
    \item \textbf{可以表征语义相似度}：得到的词向量可以较好地反应词间的语义关系。
\end{enumerate}

Word2Vec提出了两种基本模型：CBOW(连续词袋模型)和Skip-Gram（跳词模型），来构建单词的高细粒度的向量表示。
CBOW和连续Skip-gram都是对数线性语言模型。

\subparagraph{CBOW}
CBOW即Continuous Bag-Of-Words，是通过一个词的上下文来预测这个词的语义。
例如\parencite{zhang2021dive}，在文本序列“the”“man”“loves”“his”“son”中，在“loves”为中心词且上下文窗口为2的情况下，
连续词袋模型考虑基于上下文词“the”“man”“him”“son”生成中心词“loves”的条件概率，即：

\begin{equation}
    P("loves"|"the", "man", "his", "son")
\end{equation}

由于连续词袋模型中存在多个上下文词，因此在计算条件概率时对这些上下文词向量进行平均。
具体地说，对于字典中索引$i$的任意词，分别用$v_i \in \mathbb{R}^d$和$u_i \in \mathbb{R}^d$表示用作\textbf{上下文词}和\textbf{中心词}的两个向量。
给定上下文词$w_{o_1}, \cdots, w_{o_{2m}}$（在词表中索引是$o_1, \cdots, o_{2m}$）
生成任意中心词$w_c$（在词表中索引是$c$）的条件概率可以由以下公式建模：

\begin{equation}\label{eq_CBOW}
    P(w_c|w_{o_1}, \cdots, w_{o_{2m}})
    = \frac{exp \left(u^T_c(v_{o1} + \cdots + v_{o_{2m}}) / 2m \right)}
    {\sum_{i \in \mathcal{V}} exp \left( u^T_i(v_{o1} + \cdots + v_{o_{2m}}) / 2m \right)}
\end{equation}

其中词表索引集$\mathcal{V} = \{0, 1, \cdots, |\mathcal{V} - 1|\}$。
为简洁起见，我们设$\mathcal{W} = \{w_{o_1}, \cdots, w_{o_{2m}}\}$和$\overline{v}_o = (v_{o1} + \cdots + v_{o_{2m}}) / 2m $，
则\ref{eq_CBOW}可以简化为：
\begin{equation}
    P(w_c|\mathcal{W})
    = \frac{exp \left(u^T_c \overline{v}_o \right)}
    {\sum_{i \in \mathcal{V}} exp \left( u^T_i \overline{v}_o \right)}
\end{equation}

给定长度为$T$的文本序列，其中时间步t处的词表示为$w^{(t)}$。对于上下文窗口$m$，
连续词袋模型的似然函数是在给定其上下文词的情况下生成所有中心词的概率：

\begin{equation}
    \prod_{t=1}^{T} P(w^{(t)}|w^{(t-m)}, \cdots, w^{(t-1)}, w^{(t)+1}, \cdots, w^{(t+m)})
\end{equation}


\subparagraph{Skip-Gram}
Skip-Gram（跳元模型）是通过一个词语来预测上下文的词语，其思路正好与CBOW相反。
以文本序列“the”“man”“loves”“his”“son”为例。假设中心词选择“loves”，
并将上下文窗口设置为2，给定中心词“loves”，跳元模型考虑生成上下文词“the”“man”“him”“son”的条件概率：

\begin{equation}
    P("the", "man", "his", "son"|"loves")
\end{equation}

在跳元模型中，每个词都有两个$d$维向量表示，用于计算条件概率。对于词典中索引为$i$的任何词，
分别用$v_i \in \mathbb{R}^d$和$u_i \in \mathbb{R}^d$表示用作\textbf{中心词}和\textbf{上下文词}的两个向量。
给定中心词$w_c$（在词表中索引是$c$），生成任意上下文词$w_{o_1}, \cdots, w_{o_{2m}}$
（在词表中索引是$o_1, \cdots, o_{2m}$）的条件概率可以由以下公式建模：

\begin{equation}
    P(w_o|w_c)
    = \frac{exp\left(u^T_o v_c \right)}
    {\sum_{i \in \mathcal{V}} exp\left( u^T_i v_c \right)}
\end{equation}

$\mathcal{V}$的定义与CBOW模型一致。给定长度为T的文本序列，其中时间步t处的词表示为$w^{(t)}$。
假设上下文词是在给定任何中心词的情况下独立生成的。对于上下文窗口m，跳元模型的似然函数是在给定
任何中心词的情况下生成所有上下文词的概率：

\begin{equation}
    \prod_{t=1}^{T} \prod_{-m \leq j \leq m, j \neq 0} P(w^{(t+j)} | w^{(t)})
\end{equation}



\paragraph{GloVe}

\parencite{pennington2014glove}的GloVe(Global Vectors for Word Representation，全局词向量表示)
是另一种著名的基于词共现的词嵌入方法。该方法的优点是可以给予全局词汇共现统计信息学习词向量。

GloVe和Word2Vec都可以根据词语的上下文得到词向量，但是原理不同。
Word2Vec是通过上下文词预测一个单词，或根据一个词预测上下文词；GloVe则是通过“共现矩阵“计算词向量。


% 通过将矩阵归一化和平滑后的重构损耗最小化，将频率矩阵分解到较低的维数。
% [13]创建单词嵌入的方法被广泛采用，因为这种向量表示显示了组合性。
% 构成性与线性语义推理脑电信号的性质相对应，例如'Paris' - 'France' + 'Italy' = 'Rome'。


\subsection{字符嵌入(Character Embedding)}

字符级嵌入在NER中被用来捕获跨语言的形态特征。在某些NLP任务中，其对形态丰富的语言有较好的结果。
\parencite{santos2015boosting}NER应用了字符级表示以及单词嵌入，在葡萄牙语和西班牙语语料库中取得了最先进的结果。
\parencite{kim2016character}研究出了仅使用字符嵌入构建神经语言模型的正向结果。\parencite{ma2016end}利用了几种嵌入方法，包括字符三元图，
将原型信息和分层信息结合起来，用于学习NER环境中预训练的标签嵌入。中文是另一种形态学丰富的语言，
在深度学习序列标记模型中，字符嵌入比单词嵌入能表现出更好的性能\parencite{zheng2013deep}。

单词嵌入不传递字符嵌入提供的语义和其他内部信息。因此，字符嵌入能够通过将未知词的含义映射到构成字符或子词的含义来推断未知词的含义。
于是，字符嵌入解决了词汇表外(OOV)词的识别问题，如用于词性标注和语言建模或依赖性解析\parencite{ballesteros2015improved}等任务的输入语料库中不存在的单词。
字符嵌入为表示单词类型提供了一种可行的方法。字符嵌入得到了广泛的应用，因为它们避免了通过严格使用单词表示来解决OOV问题所引入的额外维度。
\parencite{chen2015joint}表明，在中文的词嵌入中引入字符嵌入会催生更有信息的词表示，例如在单词关联性和类比推理任务中有更好的结果。


\section{命名实体识别方案}

在对文本数据进行适当的预处理和字符级嵌入的基础上，将双向长短期记忆网络（BiLSTM）
和具有全局自注意层（Self-attention Layer）的门控递归单元（GRU）进行结合，
探究模型的性能。在此基础上，进行多种网络结构的组合，利用实际数据集对比探究模型优劣。


% \section{Embedding矩阵的维度选择}