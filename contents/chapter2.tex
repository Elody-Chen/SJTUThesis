% !TEX root = ../main.tex


\chapter{基于传统字符嵌入的命名实体识别方案}

\section{嵌入方案介绍}

\subsection{词嵌入(Word Embedding)}

使用预训练的词嵌入已经成为包括NER在内的NLP任务的标准特征。\parencite{collobert2011natural}提出了一种构建词嵌入的神经网络体系结构，
该体系结构构成了获取词向量表示的主要方法，用于训练NER的深度学习NLP模型。
词嵌入是由\parencite{mikolov2013efficient}首创的，他引入了连续词袋和Skip-gram模型来构建单词的高细粒度的向量表示。
\parencite{pennington2014glove}的Glove是另一种著名的基于词共现的词嵌入方法。通过将矩阵归一化和平滑后的重构损耗最小化，
将频率矩阵分解到较低的维数。[13]创建单词嵌入的方法被广泛采用，因为这种向量表示显示了组合性。
构成性与线性语义推理脑电信号的性质相对应，例如'Paris' - 'France' + 'Italy' = 'Rome'。

CBOW和连续Skip-gram都是对数线性语言模型，但它们在基本种类上有所不同。CBOW根据上下文预测目标词。
然而，连续Skip-gram模型预测给定窗口内目标词前后的单词。用作向量表示上下文的相邻单词窗口是一个需要优化的超参数。
增加窗口增加了语言模型的准确性，但也增加了考虑窗口中较远单词的计算复杂性。



\subsection{字符嵌入(Word Embedding)}

字符级嵌入在NER中被用来捕获跨语言的形态特征。在某些NLP任务中，其对形态丰富的语言有较好的结果。
\parencite{santos2015boosting}NER应用了字符级表示以及单词嵌入，在葡萄牙语和西班牙语语料库中取得了最先进的结果。
\parencite{kim2016character}研究出了仅使用字符嵌入构建神经语言模型的正向结果。\parencite{ma2016end}利用了几种嵌入方法，包括字符三元图，
将原型信息和分层信息结合起来，用于学习NER环境中预训练的标签嵌入。中文是另一种形态学丰富的语言，
在深度学习序列标记模型中，字符嵌入比单词嵌入能表现出更好的性能\parencite{zheng2013deep}。

单词嵌入不传递字符嵌入提供的语义和其他内部信息。因此，字符嵌入能够通过将未知词的含义映射到构成字符或子词的含义来推断未知词的含义。
于是，字符嵌入解决了词汇表外(OOV)词的识别问题，如用于词性标注和语言建模或依赖性解析\parencite{ballesteros2015improved}等任务的输入语料库中不存在的单词。
字符嵌入为表示单词类型提供了一种可行的方法。字符嵌入得到了广泛的应用，因为它们避免了通过严格使用单词表示来解决OOV问题所引入的额外维度。
\parencite{chen2015joint}表明，在中文的词嵌入中引入字符嵌入会催生更有信息的词表示，例如在单词关联性和类比推理任务中有更好的结果。


\section{Embedding矩阵的维度选择}


\section{命名实体识别方案}

