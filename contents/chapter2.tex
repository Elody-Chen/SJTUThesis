% !TEX root = ../main.tex


\chapter{基于词嵌入的命名实体识别方案}

\section{嵌入方案介绍}

自然语言处理任务中的一个关键问题就是如何用数学语言去表示自然语言，并且其表示的优劣则会直接影响其下游任务结果的好坏。

\subsection{词嵌入(Word Embedding)}

\paragraph{One-Hot编码}
最简单且最容易实现的的表示方法之一就是给词编码。假如能够列出所有可能出现的词，我们只需要给每个词一个唯一的编码，
即可完成简单的词表示。例如一个词表中有12个词，按照拼音首字母排列如下：

\begin{table}[!hpt]
    \caption[示例词典编码对照表]{示例词典编码对照表}
    \label{tab:case_lookup_table}
    \centering
    \begin{tabular}{lr} \toprule
      词语  &   编码 \\ \midrule
      处理  &   1 \\
      的    &   2 \\
      方案  &   3 \\
      非常  &   4 \\
      命名  &   5 \\
      难以  &   6 \\
      识别  &   7 \\
      实体  &   8 \\
      是    &   9 \\
      学习  &   10 \\
      语言  &   11 \\
      自然  &   12 \\ \bottomrule
    \end{tabular}
\end{table}

“自然语言处理非常难以学习”这句话分词后的结果为(自然, 语言, 处理, 非常, 难以, 学习)用
表~\ref{tab:case_lookup_table}的编码表示就是：(12, 11, 1, 4, 6, 10)。

在使用机器学习算法做自然语言处理任务的时候，模型所接受的输入一般是定长的向量，
所以需要将上表中的所有编码转换为向量。One-Hot编码是使用与词表等长的向量来表示特定词语，向量中的元素只包含0或1。
每个词只有在它对应的位置上元素为1，其余都为0。对于表~\ref{tab:case_lookup_table}，
其对应的One-Hot向量如表~\ref{tab:case_one_hot_lookup_table}

\begin{table}[!hpt]
    \caption[示例词典One-Hot向量对照表]{示例词典One-Hot向量对照表}
    \label{tab:case_one_hot_lookup_table}
    \centering
    \begin{tabular}{lrl} \toprule
      词语  &   编码 &  One-Hot向量 \\ \midrule
      处理	&	1	&   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	\\
      的	&	2	&	[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	\\
      方案	&	3	&	[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]	\\
      非常	&	4	&	[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]	\\
      命名	&	5	&	[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]	\\
      难以	&	6	&	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]	\\
      识别	&	7	&	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]	\\
      实体	&	8	&	[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]	\\
      是	&	9	&	[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]	\\
      学习	&	10	&	[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]	\\
      语言	&	11	&	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]	\\
      自然	&	12	&	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]	\\ \bottomrule
    \end{tabular}
\end{table}

这样表示虽然清晰，但却存在两个严重的问题：
\begin{enumerate}
    \item \textbf{向量维度过高并且高度稀疏}：自然语言处理任务中的词汇量一般非常大。
    例如中文或者英文的常用词有上万个，其他语言也类似。如此多的词语会导致One-Hot向量的维度非常大，
    计算时不仅浪费内存，计算量也极大。
    \item \textbf{信息量少，无法表征语义相似度}：自然语言的词与词之间可能存在一定的关系。
    如果我们用词向量的余弦相似度来衡量两个词的相关性，越接近1或者-1表示两个词语义越相近或者相反，
    越接近0表示语义约不相关。例如“漂亮”和“美丽”是一对近义词，它们的余弦相似度应该接近于1；
    “美丽”和“丑陋”是一对反义词，它们的余弦相似度应该接近于-1；而“美丽”和“手机”在语义上的关系较弱，
    其余弦相似度则应该接近于0。
\end{enumerate}


\paragraph{Word2Vec}

Word2Vec即word to vector，顾名思义就是将词转换为向量。该方法由\parencite{mikolov2013efficient}首创，
主要有以下特点：
\begin{enumerate}
    \item \textbf{算法效率高}：可以在百万数量级的词典和上亿规模的数据上训练；
    \item \textbf{可以表征语义相似度}：得到的词向量可以较好地反应词间的语义关系。
\end{enumerate}

Word2Vec提出了两种基本模型：CBOW(连续词袋模型)和Skip-Gram（跳词模型），来构建单词的高细粒度的向量表示。

\subparagraph{CBOW}
CBOW即Continuous Bag-Of-Words，是通过一个词的上下文来预测这个词的语义。


\subparagraph{Skip-Gram}
Skip-Gram是通过一个词语来预测上下文的词语，其思路正好与CBOW相反。


使用预训练的词嵌入已经成为包括NER在内的NLP任务的标准特征。\parencite{collobert2011natural}提出了一种构建词嵌入的神经网络体系结构，
该体系结构构成了获取词向量表示的主要方法，用于训练NER的深度学习NLP模型。
词嵌入是由\parencite{mikolov2013efficient}首创的，他引入了连续词袋和Skip-gram模型来构建单词的高细粒度的向量表示。
\parencite{pennington2014glove}的Glove是另一种著名的基于词共现的词嵌入方法。通过将矩阵归一化和平滑后的重构损耗最小化，
将频率矩阵分解到较低的维数。[13]创建单词嵌入的方法被广泛采用，因为这种向量表示显示了组合性。
构成性与线性语义推理脑电信号的性质相对应，例如'Paris' - 'France' + 'Italy' = 'Rome'。

CBOW和连续Skip-gram都是对数线性语言模型，但它们在基本种类上有所不同。CBOW根据上下文预测目标词。
然而，连续Skip-gram模型预测给定窗口内目标词前后的单词。用作向量表示上下文的相邻单词窗口是一个需要优化的超参数。
增加窗口增加了语言模型的准确性，但也增加了考虑窗口中较远单词的计算复杂性。

\subsection{字符嵌入(Character Embedding)}

字符级嵌入在NER中被用来捕获跨语言的形态特征。在某些NLP任务中，其对形态丰富的语言有较好的结果。
\parencite{santos2015boosting}NER应用了字符级表示以及单词嵌入，在葡萄牙语和西班牙语语料库中取得了最先进的结果。
\parencite{kim2016character}研究出了仅使用字符嵌入构建神经语言模型的正向结果。\parencite{ma2016end}利用了几种嵌入方法，包括字符三元图，
将原型信息和分层信息结合起来，用于学习NER环境中预训练的标签嵌入。中文是另一种形态学丰富的语言，
在深度学习序列标记模型中，字符嵌入比单词嵌入能表现出更好的性能\parencite{zheng2013deep}。

单词嵌入不传递字符嵌入提供的语义和其他内部信息。因此，字符嵌入能够通过将未知词的含义映射到构成字符或子词的含义来推断未知词的含义。
于是，字符嵌入解决了词汇表外(OOV)词的识别问题，如用于词性标注和语言建模或依赖性解析\parencite{ballesteros2015improved}等任务的输入语料库中不存在的单词。
字符嵌入为表示单词类型提供了一种可行的方法。字符嵌入得到了广泛的应用，因为它们避免了通过严格使用单词表示来解决OOV问题所引入的额外维度。
\parencite{chen2015joint}表明，在中文的词嵌入中引入字符嵌入会催生更有信息的词表示，例如在单词关联性和类比推理任务中有更好的结果。


\section{Embedding矩阵的维度选择}


\section{命名实体识别方案}

